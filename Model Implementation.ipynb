{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.engine.input_layer import Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten, Activation, UpSampling2D, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_size):\n",
    "    \n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    conv1 = Conv2D(filters=64,\n",
    "                  kernel_size=3,\n",
    "                  activation='relu',\n",
    "                  padding='same')(inputs)\n",
    "    \n",
    "    conv1 = Conv2D(filters=64,\n",
    "                  kernel_size=3, \n",
    "                  activation='relu',\n",
    "                  padding='same')(conv1)\n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size=(2,2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(filters=128,\n",
    "                  kernel_size=3,\n",
    "                  activation='relu',\n",
    "                  padding='same')(pool1)\n",
    "    \n",
    "    conv2 = Conv2D(filters=128,\n",
    "                  kernel_size=3, \n",
    "                  activation='relu',\n",
    "                  padding='same')(conv2)\n",
    "    \n",
    "    pool2 = MaxPooling2D(pool_size=(2,2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(filters=256,\n",
    "                  kernel_size=3,\n",
    "                  activation='relu',\n",
    "                  padding='same')(pool2)\n",
    "    \n",
    "    conv3 = Conv2D(filters=256,\n",
    "                  kernel_size=3, \n",
    "                  activation='relu',\n",
    "                  padding='same')(conv3)\n",
    "    \n",
    "    pool3 = MaxPooling2D(pool_size=(2,2))(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(filters=512,\n",
    "                  kernel_size=3,\n",
    "                  activation='relu',\n",
    "                  padding='same')(pool3)\n",
    "    \n",
    "    conv4 = Conv2D(filters=512,\n",
    "                  kernel_size=3, \n",
    "                  activation='relu',\n",
    "                  padding='same')(conv4)\n",
    "    \n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    \n",
    "    pool4 = MaxPooling2D(pool_size=(2,2))(drop4)\n",
    "    \n",
    "    conv5 = Conv2D(filters=1024,\n",
    "                  kernel_size=3,\n",
    "                  activation='relu',\n",
    "                  padding='same')(pool4)\n",
    "    \n",
    "    conv5 = Conv2D(filters=1024,\n",
    "                  kernel_size=3, \n",
    "                  activation='relu',\n",
    "                  padding='same')(conv5)\n",
    "    \n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    \n",
    "    up6 = UpSampling2D(size=(2,2))(drop5)\n",
    "    \n",
    "    up6 = Conv2D(filters=512,\n",
    "                  kernel_size=2,\n",
    "                  activation='relu',\n",
    "                  padding='same')(up6)\n",
    "    \n",
    "    merge6 = Concatenate(axis=3)([drop4,up6])\n",
    "    \n",
    "    conv6 = Conv2D(filters=512,\n",
    "                  kernel_size=3,\n",
    "                  activation='relu',\n",
    "                  padding='same')(merge6)\n",
    "    \n",
    "    conv6 = Conv2D(filters=512,\n",
    "                  kernel_size=3,\n",
    "                  activation='relu',\n",
    "                  padding='same')(conv6)\n",
    "    \n",
    "    up7 = UpSampling2D(size=(2,2))(conv6)\n",
    "    \n",
    "    up7 = Conv2D(filters=256,\n",
    "                  kernel_size=2,\n",
    "                  activation='relu',\n",
    "                  padding='same')(up7)\n",
    "    \n",
    "    merge7 = Concatenate(axis=3)([conv3,up7])\n",
    "    \n",
    "    conv7 = Conv2D(filters=256,\n",
    "                  kernel_size=3,\n",
    "                  activation='relu',\n",
    "                  padding='same')(merge7)\n",
    "    \n",
    "    conv7 = Conv2D(filters=256,\n",
    "                  kernel_size=3,\n",
    "                  activation='relu',\n",
    "                  padding='same')(conv7)\n",
    "    \n",
    "    up8 = UpSampling2D(size=(2,2))(conv7)\n",
    "    \n",
    "    up8 = Conv2D(filters=128,\n",
    "                  kernel_size=2,\n",
    "                  activation='relu',\n",
    "                  padding='same')(up8)\n",
    "    \n",
    "    merge8 = Concatenate(axis=3)([conv2,up8])\n",
    "    \n",
    "    conv8 = Conv2D(filters=128,\n",
    "                  kernel_size=3,\n",
    "                  activation='relu',\n",
    "                  padding='same')(merge8)\n",
    "    \n",
    "    conv8 = Conv2D(filters=128,\n",
    "                  kernel_size=3,\n",
    "                  activation='relu',\n",
    "                  padding='same')(conv8)\n",
    "    \n",
    "    up9 = UpSampling2D(size=(2,2))(conv8)\n",
    "    \n",
    "    up9 = Conv2D(filters=64,\n",
    "                  kernel_size=2,\n",
    "                  activation='relu',\n",
    "                  padding='same')(up9)\n",
    "    \n",
    "    merge9 = Concatenate(axis=3)([conv1,up9])\n",
    "    \n",
    "    conv9 = Conv2D(filters=64,\n",
    "                  kernel_size=3,\n",
    "                  activation='relu',\n",
    "                  padding='same')(merge9)\n",
    "    \n",
    "    conv9 = Conv2D(filters=64,\n",
    "                  kernel_size=3,\n",
    "                  activation='relu',\n",
    "                  padding='same')(conv9)\n",
    "    \n",
    "    conv9 = Conv2D(filters=2,\n",
    "                  kernel_size=3,\n",
    "                  activation='relu',\n",
    "                  padding='same')(conv9)\n",
    "    \n",
    "    conv10 = Conv2D(filters=2,\n",
    "                  kernel_size=3,\n",
    "                  activation='sigmoid',\n",
    "                  padding='same')(conv9)\n",
    "    \n",
    "    model = Model(inputs=inputs,outputs=conv10)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pykitti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=plt.imread(\"data_road\\\\training\\\\image_2\\\\um_000000.png\")\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train),x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 1242, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 375, 1242, 3)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=[]\n",
    "for i in range(0,10):\n",
    "    x=plt.imread(\"data_road\\\\training\\\\image_2\\\\um_00000\"+str(i)+\".png\")\n",
    "    x_train.append(x)\n",
    "'''for i in range(10,95):\n",
    "    x=plt.imread(\"data_road\\\\training\\\\image_2\\\\um_0000\"+str(i)+\".png\")\n",
    "    arr.append(x)'''\n",
    "x_train=np.array(x_train)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 375, 1242, 3)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=[]\n",
    "for i in range(0,10):\n",
    "    x=plt.imread(\"data_road\\\\training\\\\gt_image_2\\\\um_lane_00000\"+str(i)+\".png\")\n",
    "    y_train.append(x)\n",
    "y_train=np.array(y_train)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=[]\n",
    "for i in range(0,10):\n",
    "    x=plt.imread(\"data_road\\\\training\\\\gt_image_2\\\\um_lane_00000\"+str(i)+\".png\")\n",
    "    x_test.append(x)\n",
    "x_test=np.array(y_train)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 46, 155, 512), (None, 46, 154, 512)]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-ee60cf857080>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m375\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1242\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-68-df3d0ec791cd>\u001b[0m in \u001b[0;36mbuild_model\u001b[1;34m(input_size)\u001b[0m\n\u001b[0;32m     72\u001b[0m                   padding='same')(up6)\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[0mmerge6\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdrop4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mup6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     conv6 = Conv2D(filters=512,\n",
      "\u001b[1;32mc:\\users\\abhilash\\codes\\recommendation_engine\\version2\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m                                          \u001b[1;34m'You can build it manually via: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[1;32m--> 431\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhilash\\codes\\recommendation_engine\\version2\\lib\\site-packages\\keras\\layers\\merge.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    360\u001b[0m                              \u001b[1;34m'inputs with matching shapes '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m                              \u001b[1;34m'except for the concat axis. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m                              'Got inputs shapes: %s' % (input_shape))\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_merge_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 46, 155, 512), (None, 46, 154, 512)]"
     ]
    }
   ],
   "source": [
    "model=build_model((375,1242,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 1242)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=x_train[0][:,:,2]\n",
    "plt.imshow(x)\n",
    "x.shape\n",
    "x=y_train[0][:,:,2]\n",
    "plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 375, 1242, 1)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=x_train[:,:,:,1]\n",
    "x=x.reshape(10,375,1242,1)\n",
    "x.shape\n",
    "\n",
    "y=y_train[:,:,:,1]\n",
    "y=y.reshape(10,375,1242,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inputs = Input((375, 1242, 3))\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = Concatenate(axis=3)([UpSampling2D(size=(2, 2))(conv5), conv4])\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = Concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = Concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = Concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv10])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Cropping2D, concatenate,ZeroPadding2D\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop_shape(target, refer):\n",
    "        # width, the 3rd dimension\n",
    "        cw = (target.get_shape()[2] - refer.get_shape()[2]).value\n",
    "        assert (cw >= 0)\n",
    "        if cw % 2 != 0:\n",
    "            cw1, cw2 = int(cw/2), int(cw/2) + 1\n",
    "        else:\n",
    "            cw1, cw2 = int(cw/2), int(cw/2)\n",
    "        # height, the 2nd dimension\n",
    "        ch = (target.get_shape()[1] - refer.get_shape()[1]).value\n",
    "        assert (ch >= 0)\n",
    "        if ch % 2 != 0:\n",
    "            ch1, ch2 = int(ch/2), int(ch/2) + 1\n",
    "        else:\n",
    "            ch1, ch2 = int(ch/2), int(ch/2)\n",
    "\n",
    "        return (ch1, ch2), (cw1, cw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(img_shape, num_class):\n",
    "\n",
    "        concat_axis = 3\n",
    "        inputs = Input(img_shape)\n",
    "\n",
    "        conv1 = Conv2D(32, (3, 3), activation='relu', padding='same', name='conv1_1')(inputs)\n",
    "        conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        \n",
    "        conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "        conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "        conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "        conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "        conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "        conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "        pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "        conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "        conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "        up_conv5 = UpSampling2D(size=(2, 2))(conv5)\n",
    "        ch, cw = get_crop_shape(conv4, up_conv5)\n",
    "        crop_conv4 = Cropping2D(cropping=(ch,cw))(conv4)\n",
    "        up6 = concatenate([up_conv5, crop_conv4], axis=concat_axis)\n",
    "        conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "        conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "        up_conv6 = UpSampling2D(size=(2, 2))(conv6)\n",
    "        ch, cw = get_crop_shape(conv3, up_conv6)\n",
    "        crop_conv3 = Cropping2D(cropping=(ch,cw))(conv3)\n",
    "        up7 = concatenate([up_conv6, crop_conv3], axis=concat_axis) \n",
    "        conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "        conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "        up_conv7 = UpSampling2D(size=(2, 2))(conv7)\n",
    "        ch, cw = get_crop_shape(conv2, up_conv7)\n",
    "        crop_conv2 = Cropping2D(cropping=(ch,cw))(conv2)\n",
    "        up8 = concatenate([up_conv7, crop_conv2], axis=concat_axis)\n",
    "        conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "        conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "        up_conv8 = UpSampling2D(size=(2, 2))(conv8)\n",
    "        ch, cw = get_crop_shape(conv1, up_conv8)\n",
    "        crop_conv1 = Cropping2D(cropping=(ch,cw))(conv1)\n",
    "        up9 = concatenate([up_conv8, crop_conv1], axis=concat_axis)\n",
    "        conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "        conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "        ch, cw = get_crop_shape(inputs, conv9)\n",
    "        conv9 = ZeroPadding2D(padding=((ch[0], ch[1]), (cw[0], cw[1])))(conv9)\n",
    "        conv10 = Conv2D(num_class, (1, 1))(conv9)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=create_model((375,1242,3),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "             optimizer=keras.optimizers.Adadelta(),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected conv2d_150 to have shape (375, 1242, 1) but got array with shape (375, 1242, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-1556af9293da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m          \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m          \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m          verbose=1)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\abhilash\\codes\\recommendation_engine\\version2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhilash\\codes\\recommendation_engine\\version2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhilash\\codes\\recommendation_engine\\version2\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected conv2d_150 to have shape (375, 1242, 1) but got array with shape (375, 1242, 3)"
     ]
    }
   ],
   "source": [
    "model.fit(x_train,y_train,\n",
    "         batch_size=32,\n",
    "         epochs=2,\n",
    "         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
